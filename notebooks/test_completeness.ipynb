{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook to test the completeness check functionality of the IBJ project.\n",
    "\n",
    "Test on Burundi, 3rd country most visited and critical on human rights issues."
   ],
   "id": "78f6f1883f83f3cf"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# LIBRARIES ---------------------------------------------------\n",
    "import os\n",
    "os.chdir('/Users/dianaavalos/PycharmProjects/InternationalBridgesToJustice')\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from src.config import Paths\n",
    "from src.openai_utils import openai_client\n",
    "from src.chromadb_utils import load_collection\n",
    "from src.get_completeness import KeypointEvaluation, schema_completeness\n",
    "from src.query_functions import get_completeness_keypoints\n",
    "from src.file_manager import get_country_names"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the prompt and system prompt for the completeness check.",
   "id": "f76d592824220607"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "with open(Paths.PATH_FILE_PROMPT_COMPLETENESS, \"r\") as file:\n",
    "    prompt_completeness = file.read()\n",
    "\n",
    "with open(Paths.PATH_FILE_SYSTEM_PROMPT_COMPLETENESS, \"r\") as file:\n",
    "    system_prompt = file.read()\n",
    "\n",
    "print(prompt_completeness[:1000])"
   ],
   "id": "7fd1b1385626505d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(system_prompt)",
   "id": "83d4ed4184beb9ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "completeness_keypoints = get_completeness_keypoints(completeness_checklist_filepath = Paths.PATH_MD_FILE_COMPLETENESS_KEYPOINTS)\n",
    "collection = load_collection(Paths.PATH_CHROMADB, Paths.COLLECTION_NAME)\n",
    "\n",
    "country_names = get_country_names(country_names_filepath=\"data/interim/country_names_1.txt\")\n",
    "country_names = [\"Burundi\"] "
   ],
   "id": "dbc33f40811ecdfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print the keypoints to check for completeness.",
   "id": "eae973ed2fec8bdf"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "chapter = \"\"\n",
    "for country in country_names:\n",
    "    for point in tqdm(completeness_keypoints):\n",
    "        # if point is not a new chapter (look at the indentation to know)\n",
    "        indent = len(point) - len(\n",
    "            point.lstrip()\n",
    "        )\n",
    "        if indent == 0:\n",
    "            chapter = point\n",
    "        if indent > 0:\n",
    "            print(f\"\\033[93m{chapter}:\\033[0m\\033[94m{point}\\033[0m\")\n",
    "            keypoint_to_check = f\"{chapter}: {point}\""
   ],
   "id": "590ef06b5d87497c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "country = \"Burundi\"\n",
    "keypoint_to_check = \"6. Court Procedures:      4. Expert Witnesses\"\n",
    "keypoint_to_check =  \"2. Rights of the Accused:   4. Right to Medical Care\"\n",
    "\n",
    "evaluation = KeypointEvaluation(country=country, chapter=chapter, point=keypoint_to_check, system_prompt=system_prompt, model=\"gpt-4o-mini\", collection=collection, lazy=True)"
   ],
   "id": "9c716be136aa394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "evaluation.run_similarity_searches(collection=collection)\n",
    "evaluation.define_prompt(prompt_completeness=prompt_completeness)"
   ],
   "id": "d3773654e48f794c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(evaluation.prompt)",
   "id": "7aed84249c0544d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "evaluation.check_completeness(client=openai_client, temperature=0.1)",
   "id": "dbdb20b59cadc6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "evaluation.answer = json.loads(evaluation.answer) #loads the str into a dict\n",
    "type(evaluation.answer)"
   ],
   "id": "be9a66e9443546f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "evaluation.add_similarity_metadata_to_answer()",
   "id": "263639486d9ef2f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "pprint(evaluation.answer)",
   "id": "676a53fba60e9613",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we want to do exactly this in batches per country with all keypoints.",
   "id": "ceae92da1f300bdd"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "request = evaluation.build_batch_request(\n",
    "    custom_id=f\"{country}-{keypoint_to_check}\",\n",
    "    user_prompt=evaluation.prompt,\n",
    "    temperature=0.1\n",
    ")"
   ],
   "id": "1408520ffe4facba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "jsonl_file_completeness_batch = f\"{Paths.PATH_FOLDER_COMPLETENESS}/batch_input_1.jsonl\"\n",
    "with open(jsonl_file_completeness_batch, \"a\") as outfile:\n",
    "    outfile.write(json.dumps(request) + \"\\n\")\n"
   ],
   "id": "265c5159750f1c8a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
